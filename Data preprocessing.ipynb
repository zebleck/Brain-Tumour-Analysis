{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Analysis: Data Preprocessing\n",
    "\n",
    "This Jupyter notebook is part of a project focused on analyzing brain tumor data using machine learning techniques. The dataset we are working with contains histopathological images and corresponding clinical annotations of 3,115 brain tumor patients, covering a wide variety of tumor types. This rich dataset was digitized from a large dedicated brain tumor bank and made publicly available for research purposes. \n",
    "\n",
    "Our task here is twofold:\n",
    "\n",
    "1. **Data Loading**: We begin by loading the `annotation.csv` file which contains clinical annotations for each patient. This includes patient demographics, tumor characteristics (type, grade, subtype), and other relevant clinical details. Note that `label` in the code corresponds to a patient's tumor type.\n",
    "\n",
    "2. **Data Downloading and Preprocessing**: The histopathological images corresponding to each patient are hosted on the EBRAINS data proxy API. This notebook contains a script to download these `.ndpi` files. Given the irregular sizes of these images, we have implemented a mechanism to ensure that downloaded images have a minimum size (256x256, 512x512, 1024x1024, 2048x2048 pixels) for consistency and ease of further processing.\n",
    "\n",
    "The notebook also includes a utility to check which files are still missing, in case the download process is interrupted and you end up with an incomplete dataset. This allows for a more controlled download process and helps ensure data integrity.\n",
    "\n",
    "This data preprocessing step is crucial to prepare our dataset for subsequent exploratory data analysis, feature extraction, and machine learning modeling.\n",
    "\n",
    "Feel free to explore, modify, and run the cells as needed. If you have any questions or encounter any issues, please reach out to the team.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and annotations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import requests\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "with os.add_dll_directory('C://openslide-win64/bin'):\n",
    "\timport openslide\n",
    "\n",
    "# Read the CSV file containing patient information\n",
    "data_file = 'annotation.csv'\n",
    "df = pd.read_csv(data_file)\n",
    "# replace \"/\" with \"-\" in diagnosis\n",
    "df['diagnosis'] = df['diagnosis'].str.replace('/', '-')\n",
    "\n",
    "# Count the number of occurrences for each diagnosis\n",
    "diagnosis_counts = df['diagnosis'].value_counts()\n",
    "\n",
    "# Get unique diagnosis labels, replace missing values with 'nan'\n",
    "diagnosis_labels = df['diagnosis'].fillna('nan').unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Image Processing for NDPI Files \n",
    "\n",
    "This code snippet represents the image processing part of the Jupyter notebook. It includes functions for loading an NDPI file, finding the magnification level that meets the minimum size requirement, downsampling the image, and saving the processed image and its metadata as a PNG file and a JSON file, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_with_min_size(file_path, min_size=2048):\n",
    "\t# Open the NDPI file using OpenSlide\n",
    "\tndpi_file = openslide.OpenSlide(file_path)\n",
    "\tndpi_metadata = dict(ndpi_file.properties)\n",
    "\n",
    "\t# Get the number of magnification levels in the NDPI file\n",
    "\tmag_level_count = int(ndpi_metadata['openslide.level-count']) - 1\n",
    "\ttarget_mag_level = mag_level_count\n",
    "\twhile target_mag_level >= 0:\n",
    "\t\t# Get the width and height of the requested magnification level\n",
    "\t\ttarget_ndpi_width = int(ndpi_metadata[f'openslide.level[{target_mag_level}].width'])\n",
    "\t\ttarget_ndpi_height = int(ndpi_metadata[f'openslide.level[{target_mag_level}].height'])\n",
    "\t\tif target_ndpi_width >= min_size and target_ndpi_height >= min_size:\n",
    "\t\t\tbreak\n",
    "\t\ttarget_mag_level -= 1\n",
    "\tif target_mag_level < 0:\n",
    "\t\tprint('Error: NDPI file is too small')\n",
    "\t\treturn None, ndpi_metadata\n",
    "\n",
    "\ttarget_ndpi_width = int(ndpi_metadata[f'openslide.level[{target_mag_level}].width'])\n",
    "\ttarget_ndpi_height = int(ndpi_metadata[f'openslide.level[{target_mag_level}].height'])\n",
    "\n",
    "\tmag_level = target_mag_level\n",
    "\n",
    "\t# Iterate through magnification levels to find the one with a valid image\n",
    "\twhile mag_level >= 0:\n",
    "\t\t# Reopen the file after trying to read_region, otherwise you get an error\n",
    "\t\tif mag_level < target_mag_level:\n",
    "\t\t\tndpi_file = openslide.OpenSlide(file_path)\n",
    "\n",
    "\t\t# Get the width and height of the requested magnification level\n",
    "\t\tndpi_width = int(ndpi_metadata[f'openslide.level[{mag_level}].width'])\n",
    "\t\tndpi_height = int(ndpi_metadata[f'openslide.level[{mag_level}].height'])\n",
    "\t\ttry:\n",
    "            # Load the image at the requested magnification level\n",
    "\t\t\tndpi_image = ndpi_file.read_region((0, 0), mag_level, (ndpi_width, ndpi_height))\n",
    "\n",
    "            # Convert the image to RGB format\n",
    "\t\t\tndpi_image = ndpi_image.convert('RGB')\n",
    "\n",
    "\t\t\tif mag_level < target_mag_level:\n",
    "\t\t\t\t# Downsample the image to the target magnification level\n",
    "\t\t\t\tndpi_image = ndpi_image.resize((target_ndpi_width, target_ndpi_height))\n",
    "\n",
    "            # Close the NDPI file and return the image\n",
    "\t\t\tndpi_file.close()\n",
    "\t\t\treturn ndpi_image, ndpi_metadata\n",
    "\t\texcept:\n",
    "\t\t\tprint('Trying again with mag', mag_level - 1)\n",
    "\t\t\tmag_level -= 1\n",
    "\t\t\tndpi_file.close()\n",
    "\t\n",
    "\tprint(f'Error: Could not load image from {file_path} at any magnification level')\n",
    "\n",
    "def save_image(processed_path, ndpi_file_name):\n",
    "\t# Construct the file paths\n",
    "\tfile_path = os.path.join(processed_path, ndpi_file_name)\n",
    "\toutput_image_path = file_path[:-5] + '.png'\n",
    "\n",
    "\t# Get the image and metadata using the get_image_with_min_size function\n",
    "\tndpi_image, metadata = get_image_with_min_size(file_path)\n",
    "\n",
    "\t# Save the image\n",
    "\ttry:\n",
    "\t\tndpi_image.save(output_image_path)\n",
    "\texcept:\n",
    "\t\tprint(f'Didn\\'t save {output_image_path}')\n",
    "\n",
    "    # Save the metadata as a JSON file\n",
    "\tmetadata_path = os.path.join(processed_path, 'metadata')\n",
    "\tif not os.path.exists(metadata_path):\n",
    "\t\tos.mkdir(metadata_path)\n",
    "\t# Construct the metadata file path\n",
    "\tmetadata_file_path = os.path.join(metadata_path, ndpi_file_name[:-5] + '.json')\n",
    "\n",
    "\t# Write the metadata to the JSON file\n",
    "\twith open(metadata_file_path, 'w') as metadata_file:\n",
    "\t\tjson.dump(metadata, metadata_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Missing File Check\n",
    "\n",
    "The code compares the processed data files with the diagnosis labels and determines which files are missing by comparing the file counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing: 1\n",
      "Non-matching labels:\n",
      "Angiomatous meningioma: 31 32\n"
     ]
    }
   ],
   "source": [
    "def compare(compare_path):\n",
    "    missing = []\n",
    "\n",
    "    # Iterate through the diagnosis labels\n",
    "    for label in diagnosis_labels:\n",
    "        folder_path = os.path.join(compare_path, label.replace(\"/\", \"-\"))\n",
    "\n",
    "        # Count the number of files with PNG extension in the folder\n",
    "        num_files = len(glob.glob(os.path.join(folder_path, '*.png')))\n",
    "\n",
    "        # Remove the folder if it exists but has no files\n",
    "        if os.path.exists(folder_path) and num_files == 0:\n",
    "            shutil.rmtree(folder_path)\n",
    "\n",
    "        # Check if the folder exists and determine the number of present files \n",
    "        if not os.path.exists(folder_path):\n",
    "            missing.append((label, -1))\n",
    "        else:\n",
    "            missing.append((label, num_files))\n",
    "\n",
    "    return missing\n",
    "\n",
    "# IMPORTANT: set the folder path, where the processed data is stored\n",
    "folder_path = \"C:\\\\Users\\\\Kontor\\\\Github Repos\\\\Brain-Tumour-Analysis\\\\processed\"\n",
    "\n",
    "# Compare the processed data with the diagnosis labels\n",
    "missing_labels = compare(folder_path)\n",
    "\n",
    "matching_labels = []\n",
    "non_matching_labels = []\n",
    "total_missing = 0\n",
    "\n",
    "# Classify labels as matching or non-matching based on the number of files\n",
    "# This makes it easy to see which labels are still missing\n",
    "for label, num_files in missing_labels:\n",
    "    if label != 'nan':\n",
    "        if num_files == diagnosis_counts[label]:\n",
    "            matching_labels.append((label, num_files))\n",
    "        else:\n",
    "            total_missing += diagnosis_counts[label] - num_files\n",
    "            non_matching_labels.append((label, num_files))\n",
    "\n",
    "matching_labels = sorted(matching_labels, key=lambda x: x[1], reverse=True)\n",
    "non_matching_labels = sorted(non_matching_labels, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#print(\"Matching labels:\")\n",
    "#for label, num_files in matching_labels:\n",
    "#    print(f\"{label}: {num_files}\")\n",
    "#print()\n",
    "print('Total missing:', total_missing)\n",
    "print(\"Non-matching labels:\")\n",
    "for label, num_files in non_matching_labels:\n",
    "    print(f\"{label}: {num_files}\", diagnosis_counts[label])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download stream"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the full dataset is 3948.2 GB, which means we cannot download the entire dataset in one go. To solve this, we connect to the ebrains data proxy API, allowing us to download each file automatically via script. Progress is also shown while downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL and header for the API request\n",
    "url = 'https://data-proxy.ebrains.eu/api/v1/datasets/8fc108ab-e2b4-406f-8999-60269dc1f994?limit=5000'\n",
    "\n",
    "# Replace the token with your own\n",
    "token = \"eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJfNkZVSHFaSDNIRmVhS0pEZDhXcUx6LWFlZ3kzYXFodVNJZ1RXaTA1U2k0In0.eyJleHAiOjE2ODQ2ODY1OTcsImlhdCI6MTY4NDA4MTgyMiwiYXV0aF90aW1lIjoxNjg0MDgxNzk3LCJqdGkiOiJmYmI5ZDk2Ny0xYWFlLTRlMGEtYmZmYi0yMmIxYTQ3ODEwNGYiLCJpc3MiOiJodHRwczovL2lhbS5lYnJhaW5zLmV1L2F1dGgvcmVhbG1zL2hicCIsImF1ZCI6InRlYW0iLCJzdWIiOiI4NTBlNTA2Ni1mNGQwLTRjOGItYmNiYy02ZjM4ZWQzYjIzMjIiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJkYXRhLXByb3h5LWZyb250Iiwibm9uY2UiOiJlNGU0MTc4MC05NTg5LTQyYmYtODJiYi05NjVhZDBlYjRiNWYiLCJzZXNzaW9uX3N0YXRlIjoiY2ViNDI0ODctOTM4ZS00NDFlLWE4NDAtNmU5YjgzZjU4MWNmIiwiYWNyIjoiMCIsImFsbG93ZWQtb3JpZ2lucyI6WyJodHRwczovL2RhdGEtcHJveHkuZWJyYWlucy5ldSIsImh0dHBzOi8vZGF0YS1wcm94eS1wcGQuZWJyYWlucy5ldSJdLCJzY29wZSI6InByb2ZpbGUgcm9sZXMgZW1haWwgb3BlbmlkIHRlYW0iLCJzaWQiOiJjZWI0MjQ4Ny05MzhlLTQ0MWUtYTg0MC02ZTliODNmNTgxY2YiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmFtZSI6IkZhYmlhbiBLb250b3IiLCJtaXRyZWlkLXN1YiI6IjMxMDM5NiIsInByZWZlcnJlZF91c2VybmFtZSI6ImZrb250b3IiLCJnaXZlbl9uYW1lIjoiRmFiaWFuIiwiZmFtaWx5X25hbWUiOiJLb250b3IiLCJlbWFpbCI6ImYua29udG9yQHN0dWQudW5pLWhlaWRlbGJlcmcuZGUifQ.qJskDcRiyZTRF1GVqn9tptoGB8vucBTcBFIksZq_3TCnsWKHYcp5FxNUyETHf4D53FvENBS2oaZ2Qm_eqog2fBM6l7FBCmkMPULPm2AA-OMzHh2Jz0T9RmHOVaTXDbLKmXFVgSn0414Bh3FdaG8UDdBsLOmejFwhGdEgE1ztXf40viAkX7lUrtCcJDrHyZIgvFpSy68_nXtm4npPCjgHyamniI7iaI_ZZrjDQnLgDVP_JoWtXtsnyc_70uHGN2rgWUeBKPqYmAjst5f1LtiRfN9pvD9E0X9GTA9KS2VGBVcdvauhUmQmRnVEYF68jMYmWabqP4u6_flG5h-aD_teGg\"\n",
    "header = f\"Bearer {token}\"\n",
    "\n",
    "# Function to send an API request\n",
    "def request(url, header):\n",
    "\t# Send the request, including the authorization token\n",
    "\t# You can get it by inspecting the network traffic in your browser on the EBRAINS Data-Proxy website\n",
    "\tresponse = requests.get(url, headers={'Authorization': header})\n",
    "\tif response.status_code == 200:\n",
    "\t\treturn response.json()\n",
    "\telse:\n",
    "\t\tprint(response.json())\n",
    "\t\tprint(f'Request failed with status code {response.status_code}')\n",
    "\t\treturn None\n",
    "\n",
    "# Function to download a file with progress display\t\n",
    "def download(url, filename, header):\n",
    "\t\n",
    "\twith requests.get(url, headers={'Authorization': header}, stream=True) as r:\n",
    "\t\ttotal_size = int(r.headers.get('content-length', 0))\n",
    "\t\tblock_size = 1024 # 1 Kibibyte\n",
    "\n",
    "\t\t# Initialize the progress bar\n",
    "\t\tprogress_bar = tqdm(total=total_size, unit=\"iB\", unit_scale=True)\n",
    "\n",
    "\t\t# Write the downloaded data to the file while displaying the progress bar\n",
    "\t\twith open(filename, \"wb\") as f:\n",
    "\t\t\tfor chunk in r.iter_content(block_size):\n",
    "\t\t\t\tprogress_bar.update(len(chunk))\n",
    "\t\t\t\tf.write(chunk)\n",
    "\t\t\n",
    "\t\tprogress_bar.close()\n",
    "\t\tr.close()\n",
    "\t\t\n",
    "\t# Verify if the download was successful\n",
    "\tif total_size != 0 and progress_bar.n != total_size:\n",
    "\t\tprint(\"ERROR, something went wrong\")\n",
    "\t\treturn False\n",
    "\telse:\n",
    "\t\treturn True\n",
    "\n",
    "# Send request that returns information about the dataset\n",
    "data = request(url, header)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated download\n",
    "\n",
    "This code iterates over the objects in the retrieved data from the ebrains data proxy API. For each object, it extracts the label and file name, creates the necessary folder and file paths, and downloads the NDPI file if the corresponding PNG file doesn't exist. It then processes the downloaded file by converting it to PNG format and removes the NDPI file. The code keeps track of the total number of files downloaded and displays the count at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32: Starting download of Angiomatous meningioma/a1982bd3-357f-11eb-aec7-001a7dda7111.ndpi\n",
      "False\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01600050926208496,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2097152000,
       "unit": "iB",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2ca034234948db94287b2a8f3d3fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/2.10G [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to convert C:\\Users\\Kontor\\Github Repos\\Brain-Tumour-Analysis\\processed\\Angiomatous meningioma\\a1982bd3-357f-11eb-aec7-001a7dda7111.ndpi\n",
      "Finished downloading 1 files\n"
     ]
    }
   ],
   "source": [
    "total_downloads = 0\n",
    "\n",
    "# The base url we will use to download the files\n",
    "api_base_url = 'https://data-proxy.ebrains.eu/api/v1/datasets/8fc108ab-e2b4-406f-8999-60269dc1f994/'\n",
    "\n",
    "# Iterate over each \"object\" in the dataset, which are the files\n",
    "for obj in data['objects']:\n",
    "\tname = obj['name'].replace('v1.0/', '')\n",
    "\tname = name.replace('Embryonal tumour with multilayered rosette, C19MC-altered', 'Embryonal tumour with multilayered rosettes, C19MC-altered')\n",
    "\t\n",
    "\tif name == 'annotation.csv':\n",
    "\t\tcontinue\n",
    "\t\n",
    "\t# Get the label of the current object (i.e. \"Embryonal carcinoma\")\n",
    "\t# and the file (i.e. \"a1980534-357f-11eb-a65a-001a7dda7111.ndpi\")\n",
    "\tlabel, file = name.split('/')\n",
    "\n",
    "\t# Create a folder path for the current label if it doesn't exist yet\n",
    "\tdata_folder_path = os.path.join(folder_path, label)\n",
    "\tif not os.path.exists(data_folder_path):\n",
    "\t\tos.makedirs(data_folder_path)\n",
    "\n",
    "\tfile_path = os.path.join(data_folder_path, file)\n",
    "\tpng_path = file_path.replace('.ndpi', '.png')\n",
    "\n",
    "\t# Skip the current object if the label is not present in diagnosis_counts\n",
    "\tif not label in diagnosis_counts and 'Control' not in label:\n",
    "\t\tcontinue\n",
    "\n",
    "\t# Check if the PNG file already exists, otherwise start the download\n",
    "\tif not os.path.exists(png_path):\n",
    "\t\tn_files = len(glob.glob(os.path.join(data_folder_path, '*.png')))\n",
    "\t\tif 'Control' in label:\n",
    "\t\t\tprint(f'{n_files+1}/47: Starting download of', name)\n",
    "\t\telse:\n",
    "\t\t\tprint(f'{n_files+1}/{diagnosis_counts[label]}: Starting download of', name)\n",
    "\t\tprint(os.path.exists(file_path))\n",
    "\n",
    "\t\t# If the file already exists, it means the conversation to PNG failed, so we skip\n",
    "\t\tif not os.path.exists(file_path):\n",
    "\t\t\t# Try to download. If it success, we try to convert to PNG.\n",
    "\t\t\tif download(api_base_url + 'v1.0/' + name, file_path, header):\n",
    "\t\t\t\ttotal_downloads += 1\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tsave_image(data_folder_path, file)\n",
    "\t\t\t\t\tos.remove(file_path)\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprint('Failed to convert', file_path)\n",
    "\n",
    "print('Finished downloading', total_downloads, 'files')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize\n",
    "\n",
    "The following script converts the dataset into one of regular size, by iterating over the files in the `processed` folder, loading each image, and resizing it to the targeted size. It then saves the resized image in the `resized` folder. We choose sizes of 512x512, 1024x1024, and 2048x2048 pixels, as these are common sizes used in image processing and machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def resize_images(folder_path, target_size, output_folder):\n",
    "    # Create the output folder based on the target size\n",
    "    output_folder = os.path.join(output_folder, f\"{target_size[0]}x{target_size[1]}\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "     # Traverse through the data folder and its subfolders\n",
    "    for root, subfolders, files in os.walk(folder_path):\n",
    "        for subfolder in subfolders:\n",
    "            subfolder_path = os.path.join(root, subfolder)\n",
    "            output_subfolder_path = os.path.join(output_folder, os.path.relpath(subfolder_path, folder_path))\n",
    "\n",
    "            # Copy the 'metadata' subfolder as-is without resizing\n",
    "            if 'metadata' in subfolder and not os.path.exists(output_subfolder_path):\n",
    "                shutil.copytree(subfolder_path, output_subfolder_path)\n",
    "                continue\n",
    "\n",
    "            # Create the output subfolder if it doesn't exist\n",
    "            if not os.path.exists(output_subfolder_path):\n",
    "                os.makedirs(output_subfolder_path)\n",
    "            \n",
    "            # Resize each PNG image in the subfolder\n",
    "            for file in os.listdir(subfolder_path):\n",
    "                if file.endswith(\".png\"):\n",
    "                    file_path = os.path.join(subfolder_path, file)\n",
    "                    output_file_path = os.path.join(output_subfolder_path, file)\n",
    "                    try:\n",
    "                         # Open the image, resize it, and save the resized image\n",
    "                        with Image.open(file_path) as img:\n",
    "                            resized_img = img.resize(target_size)\n",
    "                            resized_img.save(output_file_path)\n",
    "                    except:\n",
    "                        # Print an error message if resizing fails\n",
    "                        print(f'Error: Could not resize {file_path}')\n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\Kontor\\\\Github Repos\\\\Brain-Tumour-Analysis\\\\processed\"\n",
    "output_path = \"C:\\\\Users\\\\Kontor\\\\Github Repos\\\\Brain-Tumour-Analysis\\\\resized\"\n",
    "\n",
    "resize_images(folder_path, (256, 256), output_path)\n",
    "resize_images(folder_path, (512, 512), output_path)\n",
    "resize_images(folder_path, (1024, 1024), output_path)\n",
    "resize_images(folder_path, (2048, 2048), output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
